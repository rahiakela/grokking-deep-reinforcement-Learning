{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-bandit-walk-example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMi89h2+/eyZj8jL09rhK4Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-reinforcement-Learning/blob/main/2-mathematical-foundations-of-reinforcement-learning/1_bandit_walk_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACV5O1PShejb"
      },
      "source": [
        "## Bandit walk example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwUB1T8Lhtj0"
      },
      "source": [
        "BW is a simple grid-world (GW) environment. GWs are a common type of environment for studying RL algorithms that are grids of any size. GWs can have any model (transition and reward functions) you can think of and can make any kind of actions available.\n",
        "\n",
        "But, they all commonly make move actions available to the agent: \n",
        "- Left, \n",
        "- Down, \n",
        "- Right, \n",
        "- Up \n",
        "\n",
        "(or West, South, East, North, which is more precise because the agent has no heading and usually has no visibility of the full grid, but cardinal directions can also be more confusing).\n",
        "\n",
        "And, of course, each action corresponds with its logical transition: Left goes left, and Right goes right. Also, they all tend to have a fully observable discrete state and observation spaces (that is, state equals observation) with integers representing the cell id location of the agent.\n",
        "\n",
        "A “walk” is a special case of grid-world environments with a single row. In reality, what I call a “walk” is more commonly referred to as a “Corridor.” But, I use the term “walk” for all the grid-world environments with a single row.\n",
        "\n",
        "The bandit walk (BW) is a walk with three states, but only one non-terminal state. Environments that have a single non-terminal state are called “bandit” environments. “Bandit” here is an analogy to slot machines, which are also known as “one-armed bandits”; they have one arm and, if you like gambling, can empty your pockets, the same way a bandit would.\n",
        "\n",
        "The BW environment has just two actions available: a Left (action 0) and an Right (action 1) action. BW has a deterministic transition function: a Left action always moves the agent to the Left, and a Right action always moves the agent to the right. The reward signal is a +1 when landing on the rightmost cell, 0 otherwise. The agent starts in the middle cell.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bw-environment.png?raw=1' width='800'/>\n",
        "\n",
        "A graphical representation of the BW environment would look like the following.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bw-graph.png?raw=1' width='800'/>\n",
        "\n",
        "We can also represent this environment in a table form.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bw-table.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pRUOv46sBzU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZSZZLQdsDqk"
      },
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hnHCeasJzE"
      },
      "source": [
        "import gym, gym_walk"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDHl8_vsPWy"
      },
      "source": [
        "## Bandit Walk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EfXPkb_lSWL",
        "outputId": "f4adbdec-7b6a-46b7-d799-da297cbe27d3"
      },
      "source": [
        "############## Bandit Walk ###############################\n",
        "# deterministic environment (100% action success)\n",
        "# 1-non-terminal states, 2-terminal states\n",
        "# only reward is still at the right-most cell in the \"walk\"\n",
        "# episodic environment, the agent terminates at the left- or right-most cell (after 1 action selection -- any action)\n",
        "# agent starts in state 1 (middle of the walk) T-1-T\n",
        "# actions left (0) or right (1)\n",
        "\n",
        "P = {\n",
        "    0: {\n",
        "        0: [(1.0, 0, 0.0, True)],\n",
        "        1: [(1.0, 0, 0.0, True)]\n",
        "    },\n",
        "    1: {\n",
        "        0: [(1.0, 0, 0.0, True)],\n",
        "        1: [(1.0, 2, 0.0, True)]\n",
        "    },\n",
        "    2: {\n",
        "        0: [(1.0, 2, 0.0, True)],\n",
        "        1: [(1.0, 2, 0.0, True)]\n",
        "    }\n",
        "}\n",
        "\n",
        "P"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n",
              " 1: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 2, 0.0, True)]},\n",
              " 2: {0: [(1.0, 2, 0.0, True)], 1: [(1.0, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZc5g6gnwmX",
        "outputId": "a21430cf-ce6b-4706-d58c-6145d4ffb545"
      },
      "source": [
        "P = gym.make(\"BanditWalk-v0\").env.P\n",
        "P"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)],\n",
              "  1: [(1.0, 0, 0.0, True), (0.0, 0, 0.0, True), (0.0, 0, 0.0, True)]},\n",
              " 1: {0: [(1.0, 0, 0.0, True), (0.0, 1, 0.0, False), (0.0, 2, 1.0, True)],\n",
              "  1: [(1.0, 2, 1.0, True), (0.0, 1, 0.0, False), (0.0, 0, 0.0, True)]},\n",
              " 2: {0: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)],\n",
              "  1: [(1.0, 2, 0.0, True), (0.0, 2, 0.0, True), (0.0, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A4aKXNpssnd"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bw-code.png?raw=1' width='800'/>"
      ]
    }
  ]
}