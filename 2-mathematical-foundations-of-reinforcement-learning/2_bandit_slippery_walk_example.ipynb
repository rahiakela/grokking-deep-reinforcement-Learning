{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-bandit-slippery-walk-example.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOB77EALE7AoLrXVRJR0e+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/grokking-deep-reinforcement-Learning/blob/main/2-mathematical-foundations-of-reinforcement-learning/2_bandit_slippery_walk_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACV5O1PShejb"
      },
      "source": [
        "## Bandit slippery walk example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwUB1T8Lhtj0"
      },
      "source": [
        "BW is a simple grid-world (GW) environment. GWs are a common type of environment for studying RL algorithms that are grids of any size. GWs can have any model (transition and reward functions) you can think of and can make any kind of actions available.\n",
        "\n",
        "But, they all commonly make move actions available to the agent: \n",
        "- Left, \n",
        "- Down, \n",
        "- Right, \n",
        "- Up \n",
        "\n",
        "(or West, South, East, North, which is more precise because the agent has no heading and usually has no visibility of the full grid, but cardinal directions can also be more confusing).\n",
        "\n",
        "And, of course, each action corresponds with its logical transition: Left goes left, and Right goes right. Also, they all tend to have a fully observable discrete state and observation spaces (that is, state equals observation) with integers representing the cell id location of the agent.\n",
        "\n",
        "A “walk” is a special case of grid-world environments with a single row. In reality, what I call a “walk” is more commonly referred to as a “Corridor.” But, I use the term “walk” for all the grid-world environments with a single row.\n",
        "\n",
        "Let’s say the surface of the walk is slippery and each action has a 20% chance of sending the agent backwards. I call this environment the bandit slippery walk (BSW).\n",
        "\n",
        "BSW is still a one-row-grid world, a walk, a corridor, with only Left and Right actions available. Again, three states and two actions. The reward is the same as before, +1 when landing at the rightmost state (except when coming from the rightmost state-from itself), and zero otherwise.\n",
        "\n",
        "However, the transition function is different: 80% of the time the agent moves to the intended cell, and 20% of time in the opposite direction.\n",
        "\n",
        "A depiction of this environment would look as follows.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bsw-environment.png?raw=1' width='800'/>\n",
        "\n",
        "Identical to the BW environment! Interesting . . .\n",
        "\n",
        "How do we know that the action effects are stochastic? How do we represent the “slippery” part of this problem? The graphical and table representations can help us with that.\n",
        "\n",
        "A graphical representation of the BSW environment would look like the following.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bsw-graph.png?raw=1' width='800'/>\n",
        "\n",
        "The BSW environment has a stochastic transition function. Let’s now represent this environment in a table form as well.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bsw-table.png?raw=1' width='800'/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pRUOv46sBzU"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZSZZLQdsDqk"
      },
      "source": [
        "!pip install git+https://github.com/mimoralea/gym-walk#egg=gym-walk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_hnHCeasJzE"
      },
      "source": [
        "import gym, gym_walk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPDHl8_vsPWy"
      },
      "source": [
        "## Bandit Slippery Walk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EfXPkb_lSWL",
        "outputId": "14d807e0-e396-44d5-e349-a48e4bb8f89f"
      },
      "source": [
        "############## Bandit Slippery Walk ###############################\n",
        "# stochastic environment (80% action success, 20% backwards)\n",
        "# 1-non-terminal states, 2-terminal states\n",
        "# only reward is still at the right-most cell in the \"walk\"\n",
        "# episodic environment, the agent terminates at the left- or right-most cell (after 1 action selection -- any action)\n",
        "# agent starts in state 1 (middle of the walk) T-1-T\n",
        "# actions left (0) or right (1)\n",
        "\n",
        "P = {\n",
        "    0: {\n",
        "        0: [(1.0, 0, 0.0, True)],\n",
        "        1: [(1.0, 0, 0.0, True)]\n",
        "    },\n",
        "    1: {\n",
        "        0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n",
        "        1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]\n",
        "    },\n",
        "    2: {\n",
        "        0: [(1.0, 2, 0.0, True)],\n",
        "        1: [(1.0, 2, 0.0, True)]\n",
        "    }\n",
        "}\n",
        "\n",
        "P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(1.0, 0, 0.0, True)], 1: [(1.0, 0, 0.0, True)]},\n",
              " 1: {0: [(0.8, 0, 0.0, True), (0.2, 2, 1.0, True)],\n",
              "  1: [(0.8, 2, 1.0, True), (0.2, 0, 0.0, True)]},\n",
              " 2: {0: [(1.0, 2, 0.0, True)], 1: [(1.0, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoZc5g6gnwmX",
        "outputId": "4ba357a9-e658-4316-c753-b7cd06b50ad3"
      },
      "source": [
        "P = gym.make(\"BanditSlipperyWalk-v0\").env.P\n",
        "P"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: {0: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)],\n",
              "  1: [(0.8, 0, 0.0, True), (0.0, 0, 0.0, True), (0.2, 0, 0.0, True)]},\n",
              " 1: {0: [(0.8, 0, 0.0, True), (0.0, 1, 0.0, False), (0.2, 2, 1.0, True)],\n",
              "  1: [(0.8, 2, 1.0, True), (0.0, 1, 0.0, False), (0.2, 0, 0.0, True)]},\n",
              " 2: {0: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)],\n",
              "  1: [(0.8, 2, 0.0, True), (0.0, 2, 0.0, True), (0.2, 2, 0.0, True)]}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A4aKXNpssnd"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/reinforcement-learning/bsw-code.png?raw=1' width='800'/>"
      ]
    }
  ]
}